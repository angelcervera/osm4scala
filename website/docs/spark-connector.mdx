---
title: Spark SQL Connector
---



## Overview
Using *Osm4scala Spark SQL Connector*, reading OSM Pbf file from **PySpark**, **Spark Scala**, **SparkSQL** or **SparkR** is
so easy as writing `.read.format("osm.pbf")`.

The current implementation offers:

- Spark 2 and 3 versions, with Scala 2.11 and 2.12
- Full Spark SQL integration.
- Easy schema.
- Internal optimizations, like:
    - Transparent parallelism reading multiply pbf files.
    - File splitting to increase parallelism per pbf file.
    - Pushdown required columns.

The library is distributed via [Maven Repo](https://mvnrepository.com/artifact/com.acervera.osm4scala) in two different ways,
[all in one jar](#all-in-on-jar) to be able to use directly with all dependencies or as [plain scala dependency](#plain-non-shaded-jar-dependency).

## Schema definition

The Dataframe Schema used is the following one:
```
StructType(
    StructField(id,LongType,false),
    StructField(type,ByteType,false),
    StructField(latitude,DoubleType,true),
    StructField(longitude,DoubleType,true),
    StructField(nodes,ArrayType(LongType,true),true),
    StructField(relations,ArrayType(StructType(
        StructField(id,LongType,true),
        StructField(relationType,ByteType,true),
        StructField(role,StringType,true)
    ),true),true),
    StructField(tags,MapType(StringType,StringType,true),true)
)
```
Where the column `type` could be:

| value |  meaning |
|:-----:|:--------:|
| 0     | Node     |
| 1     | Way      |
| 2     | Relation |

Where the column `relationType` could be:

| value |  meaning |
|:-----:|:--------:|
| 0     | Node     |
| 1     | Way      |
| 2     | Relation |
| 3     | Unrecognized |

## All in one jar
Usually, osm4scala is used from the [Spark Shell](spark-shell) or from a [Notebook](notebook). For these cases,
to simplify the way to add the connector as dependency, you have a shaded fat jar version with all dependencies that are
necessary.
The fat jar is near 5MB, so the size should be not a problem.

As you probably know, Spark is base in Scala. Different Spark distributions are using different Scala versions.
This is the Spark/Scala version combination available for latest release v1.0.7:

| Spark |  Scala | Packages |
|:-----:|:------:|:---------|
| 2.4   | 2.11   | [`com.acervera.osm4scala:osm4scala-spark2-shaded_2.11:1.0.7`](https://search.maven.org/artifact/com.acervera.osm4scala/osm4scala-spark2-shaded_2.11/1.0.7/jar)
| 2.4   | 2.12   | [`com.acervera.osm4scala:osm4scala-spark2-shaded_2.12:1.0.7`](https://search.maven.org/artifact/com.acervera.osm4scala/osm4scala-spark2-shaded_2.12/1.0.7/jar)
| 3.0   | 2.12   | [`com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.7`](https://search.maven.org/artifact/com.acervera.osm4scala/osm4scala-spark3-shaded_2.12/1.0.7/jar)

Although following sections are focus on Spark Shell and Notebooks, you can use the same technique in other situations where
you want to use the shaded version.

### Why a fat shaded library?

Osm4scala has a transitive dependency with Java Google Protobuf library. Spark, Hadoop and other libraries in the
ecosystem are using an old version of the same library (currently v2.5.0 from Mar, 2013) that is not compatible.

To solve the conflict, I published the library in two fashion:
- Fat and Shaded as `osm4scala-spark3-shaded` that solves `com.google.protobuf.**` conflicts.
- Don't shaded as `osm4scala-spark3`, so you can solve the conflict on your way.

### Spark Shell

1. Start the spark shell as usual, using the `--package` option to add the right dependency. The dependency will depend to
   the Spark Version that you are using. Please, check the reference table in the previous section.
    ```shell title="Scala"
    bin/spark-shell --packages 'com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.7'
    ```
    ```scala title="PySpark"
    bin/pyspark --packages 'com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.7'
    ```
    ```scala title="SQL"
    bin/spark-sql --packages 'com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.7'
    ```

2. Create the Dataframe using the osm.pbf format, pointing to the pbf file or folder containing pbf files.
    ```scala title="Scala"
    scala> val osmDF = spark.read.format("osm.pbf").load("<osm files path here>")
    ```
    ```python title="PySpark"
    >>> osmDF = spark.read.format("osm.pbf").load("<osm files path here>")
    ```
    ```sql title="SQL"
    spark-sql> CREATE TABLE osm USING osm.pbf LOCATION '<osm files path here>';
    ```

3. Use the created dataframe as usual, keeping in mind the schema explained previously.

   - In the next example, we are going to count the number of different primitives in
   the file. As explained in the schema, 0 are nodes, 1 ways and 2 relations.
    ```scala title="Scala"
    scala> osmDF.groupBy("type").count().show()
    +----+--------+
    |type|   count|
    +----+--------+
    |   1| 2096455|
    |   2|   91971|
    |   0|19426617|
    +----+--------+
    ```
    ```python title="PySpark"
    >>> osmDF.groupBy("type").count().show()
    +----+--------+
    |type|   count|
    +----+--------+
    |   1| 2096455|
    |   2|   91971|
    |   0|19426617|
    +----+--------+
    ```
    ```sql title="SQL"
    spark-sql> select type, count(type) from osm group by type
    1   338795
    2   10357
    0   2328075
    ```

    - In this other examples, we are going to extract all traffic lights as POIs.
    ```scala title="Scala"
    scala> osmDF.select("latitude", "longitude", "tags").where("element_at(tags, 'highway') == 'traffic_signals'").show(10,false)
    +------------------+-------------------+------------------------------------------------------------------------------+
    |latitude          |longitude          |tags                                                                          |
    +------------------+-------------------+------------------------------------------------------------------------------+
    |54.59766649999997 |-5.8889806000000045|[highway -> traffic_signals]                                                  |
    |54.58006689999997 |-5.938683200000003 |[highway -> traffic_signals, traffic_signals -> signal]                       |
    |54.58260049999997 |-5.946187600000005 |[direction -> backward, highway -> traffic_signals, traffic_signals -> signal]|
    |51.90097769999996 |-8.470285700000005 |[highway -> traffic_signals]                                                  |
    |51.901616299999965|-8.470139700000004 |[highway -> traffic_signals]                                                  |
    |51.89978239999997 |-8.465829200000002 |[highway -> traffic_signals]                                                  |
    |51.89707529999997 |-8.474892800000001 |[highway -> traffic_signals]                                                  |
    |51.89784849999997 |-8.466895200000002 |[highway -> traffic_signals]                                                  |
    |51.89547809999997 |-8.476100900000002 |[highway -> traffic_signals]                                                  |
    |51.89772569999997 |-8.477145100000003 |[highway -> traffic_signals]                                                  |
    +------------------+-------------------+------------------------------------------------------------------------------+
    only showing top 10 rows
    ```
    ```python title="PySpark"
    >>> osmDF.select("latitude", "longitude", "tags").where("element_at(tags, 'highway') == 'traffic_signals'").show(10,False)
    +------------------+-------------------+------------------------------------------------------------------------------+
    |latitude          |longitude          |tags                                                                          |
    +------------------+-------------------+------------------------------------------------------------------------------+
    |54.59766649999997 |-5.8889806000000045|[highway -> traffic_signals]                                                  |
    |54.58006689999997 |-5.938683200000003 |[highway -> traffic_signals, traffic_signals -> signal]                       |
    |54.58260049999997 |-5.946187600000005 |[direction -> backward, highway -> traffic_signals, traffic_signals -> signal]|
    |51.90097769999996 |-8.470285700000005 |[highway -> traffic_signals]                                                  |
    |51.901616299999965|-8.470139700000004 |[highway -> traffic_signals]                                                  |
    |51.89978239999997 |-8.465829200000002 |[highway -> traffic_signals]                                                  |
    |51.89707529999997 |-8.474892800000001 |[highway -> traffic_signals]                                                  |
    |51.89784849999997 |-8.466895200000002 |[highway -> traffic_signals]                                                  |
    |51.89547809999997 |-8.476100900000002 |[highway -> traffic_signals]                                                  |
    |51.89772569999997 |-8.477145100000003 |[highway -> traffic_signals]                                                  |
    +------------------+-------------------+------------------------------------------------------------------------------+
    only showing top 10 rows
    ```
    ```sql title="SQL"
    spark-sql> select latitude, longitude, tags from osm where type = 0 and element_at(tags, "highway") == 'traffic_signals' limit 10;
    40.42125            -3.6844500000000004 {"crossing":"traffic_signals","crossing_ref":"zebra","highway":"traffic_signals"}
    40.41779000000001   -3.6241199999999996 {"highway":"traffic_signals"}
    40.41473000000003   -3.627109999999999  {"highway":"traffic_signals"}
    40.414200000000015  -3.6282099999999993 {"highway":"traffic_signals"}
    40.42635999999994   -3.727220000000005  {"crossing":"traffic_signals","highway":"traffic_signals"}
    40.41937999999995   -3.688820000000004  {"highway":"traffic_signals"}
    40.426489999999944  -3.687640000000004  {"highway":"traffic_signals","traffic_signals":"signal"}
    40.421339999999944  -3.683020000000004  {"highway":"traffic_signals"}
    40.41797999999994   -3.669340000000004  {"highway":"traffic_signals"}
    40.418319999999945  -3.6762200000000043 {"highway":"traffic_signals","traffic_signals":"signal"}
    Time taken: 0.128 seconds, Fetched 10 row(s)
    ```

### Notebook

There are different notebooks solutions in the market and each one is using a different way to import libraries. But after
importing the library, you can use the osm4scala connector in the same way.

For this section, we are going to talk about [Jupyter Notebook](https://jupyter.org). If you can not access to a Jupyter Notebook
installation, you can use [jupyter/all-spark-notebook](https://hub.docker.com/r/jupyter/all-spark-notebook) Docker image as I will do.
[Here](https://jupyter-docker-stacks.readthedocs.io/en/latest/index.html), full documentation about how to install and use it.

Also, You can use [MyBinder](https://mybinder.org/v2/gh/jupyter/docker-stacks/master?filepath=README.ipynb).


### Spark application

When we need to write more complex analysis, data extractions, ETLs, etc, it is necessary to write Spark applications.

1. Import the spark connector **it is not really necessary** because the integration is transparent.

   Only two *possible* advantages (not available if using Python) are:
   - The use of static constants, for example, to avoid `magic numbers` for primitive and relation types.
   - Using the library as part of Unit Testing or Integration Testing.
   - Adding osm4scala jar library as part of the deployable artifact.

    **For Python**, like in Scala, it is not necessary to import the library except in runtime. But unlike in Scala, you
    can not *easily* to import and use facilities from the Scala library. So in this case, you can jump to the next step.

    ```sbt title="Sbt"
    libraryDependencies += "com.acervera.osm4scala" % "osm4scala-spark3-shaded_2.12" % "1.0.7"
    ```
    ```xml title="Maven"
    <dependency>
        <groupId>com.acervera.osm4scala</groupId>
        <artifactId>osm4scala-spark3-shaded_2.12</artifactId>
        <version>1.0.7</version>
    </dependency>
    ```
    :::tip Reduce artifact size.

    The shaded dependency is near 5MB. You can add this dependency as a package when you submit the job instead to include it
    in the deployable artifact generated. To do it, set the scope dependency as `Test` or `Provided`.

    *If you don't know what I'm talking about, don't pay too much attention and forget it.* :wink:

    :::

2. Create the Dataframe using the osm.pbf format, pointing to the pbf file or folder containing pbf files, and use as usual.
    ```scala title="Scala / PrimitivesCounter.scala"

    import com.acervera.osm4scala.spark.OsmSqlEntity
    import org.apache.spark.sql.SparkSession

    object PrimitivesCounter {

      def main(args: Array[String]): Unit = {
        val spark = SparkSession
          .builder()
          .appName("Primitives counter")
          .getOrCreate()

        spark.read
          .format("osm.pbf")
          .load(args(0))
          .groupBy(OsmSqlEntity.FIELD_TYPE)
          .count
          .show
      }
    }

    ```
    ```python title="PySpark / PrimiriveCounter.py"
    from pyspark.sql import SparkSession
    import sys

    if __name__ == '__main__':

        spark = SparkSession.builder.appName("Primitives counter").getOrCreate()

        spark.read.format("osm.pbf")\
            .load(sys.argv[1])\
            .groupBy("type")\
            .count()\
            .show()
    ```

3. Submit the application to your Spark cluster.
    ```shell title="Scala"
    bin/spark-submit \
        --packages 'com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.7' \
        examples/spark-documentation/target/scala-2.12/osm4scala-examples-spark-documentation_2.12-1.0.8-SNAPSHOT.jar \
        /tmp/osm/monaco-latest.osm.pbf
    ```
    ```shell title="PySpark"
    bin/spark-submit \
        --packages 'com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.7' \
        examples/spark-documentation/src/main/scala/com/acervera/osm4scala/examples/spark/documentation/PrimiriveCounter.py \
        /tmp/osm/monaco-latest.osm.pbf
    ```

    :::note Optional --packages.

    You will not need to add `--packages 'com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.7'` if it is part of the
    deployed artifact.

    :::














## Plain (non-shaded jar) dependency.
When we need to write more complex analysis, data extractions, ETLs, etc, it is necessary to write Spark applications. In
this case, the best practice is to manage dependencies using `sbt` or `maven`, instead to import the shaded file.

As you probably know, Spark is base in Scala. Different Spark distributions are using different Scala versions.

:::note

OSM Pbf files are based on [Protocol Buffer](https://developers.google.com/protocol-buffers), so [Scalapb](https://scalapb.github.io) is
used as deserializer. In the next version table, I added one column with the version used for each combination.

:::

This is the Spark/Scala version combination available for latest release v1.0.7:

| Spark | Scalapb |  Scala | Packages |
|:-----:|:-------:|:------:|:---------|
| 2.4   | 0.9.7  | 2.11   | [`com.acervera.osm4scala:osm4scala-spark2_2.11:1.0.7`](https://search.maven.org/artifact/com.acervera.osm4scala/osm4scala-spark2_2.11/1.0.7/jar)
| 2.4   | 0.10.2 | 2.12   | [`com.acervera.osm4scala:osm4scala-spark2_2.12:1.0.7`](https://search.maven.org/artifact/com.acervera.osm4scala/osm4scala-spark2_2.12/1.0.7/jar)
| 3.0   | 0.10.2 | 2.12   | [`com.acervera.osm4scala:osm4scala-spark3_2.12:1.0.7`](https://search.maven.org/artifact/com.acervera.osm4scala/osm4scala-spark3_2.12/1.0.7/jar)

After importing the connector, you can use it as we explained in the [All in one section](#all-in-one-jar). So lets see
how to import the library in our project and few examples:

### Resolving dependency conflicts


