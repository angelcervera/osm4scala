---
title: Spark SQL Connector
---



## Overview
Using *Osm4scala Spark SQL Connector*, reading OSM Pbf file from **PySpark**, **Spark Scala**, **SparkSQL** or **SparkR** is
so easy as writing `.read.format("osm.pbf")`.

The current implementation offers:

- Spark 2 and 3 versions, with Scala 2.11 and 2.12
- Full Spark SQL integration.
- Easy schema.
- Internal optimizations, like:
    - Transparent parallelism reading multiply pbf files.
    - File splitting to increase parallelism per pbf file.
    - Pushdown required columns.

The library is distributed via [Maven Repo](https://mvnrepository.com/artifact/com.acervera.osm4scala) in two different ways,
[all in one jar](#all-in-on-jar) to be able to use directly with all dependencies or as [scala dependency](#as-scala-dependency).

## Schema definition

The Dataframe Schema used is the following one:
```
StructType(
    StructField(id,LongType,false),
    StructField(type,ByteType,false),
    StructField(latitude,DoubleType,true),
    StructField(longitude,DoubleType,true),
    StructField(nodes,ArrayType(LongType,true),true),
    StructField(relations,ArrayType(StructType(
        StructField(id,LongType,true),
        StructField(relationType,ByteType,true),
        StructField(role,StringType,true)
    ),true),true),
    StructField(tags,MapType(StringType,StringType,true),true)
)
```
Where the column `type` could be:

| value |  meaning |
|:-----:|:--------:|
| 0     | Node     |
| 1     | Way      |
| 2     | Relation |

Where the column `relationType` could be:

| value |  meaning |
|:-----:|:--------:|
| 0     | Node     |
| 1     | Way      |
| 2     | Relation |
| 3     | Unrecognized |

## All in one jar
Usually, osm4scala is used from the [Spark Shell](spark-shell) or from a [Notebook](notebook). For those cases, to simplify the way to add the
connector as dependency, you have a shaded fat jar version with all dependencies that are necessary.
The fat jar is around 4MB, so the size should be not a problem.

As you probably know, Spark is base in Scala. Different Spark distributions are using different Scala versions.
This is the Spark/Scala version combination available for latest release v1.0.7:

| Spark |  Scala | Packages |
|:-----:|:------:|---------:|
| 2.4   | 2.11   | [`com.acervera.osm4scala:osm4scala-spark2-shaded_2.11:1.0.7`](https://search.maven.org/artifact/com.acervera.osm4scala/osm4scala-spark2_2.11/1.0.7/jar)
| 2.4   | 2.12   | [`com.acervera.osm4scala:osm4scala-spark2-shaded_2.12:1.0.7`](https://search.maven.org/artifact/com.acervera.osm4scala/osm4scala-spark2_2.12/1.0.7/jar)
| 3.0   | 2.12   | [`com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.7`](https://search.maven.org/artifact/com.acervera.osm4scala/osm4scala-spark3_2.12/1.0.7/jar)

Although following sections are focus on Spark Shell and Notebooks, you can use the same technique in other situations where
you want to use the shaded version.

### Spark Shell

1. Start the spark shell as usual, using the `--package` option to add the right dependency. The dependency will depend to
   the Spark Version that you are using. Please, check the reference table in the previous section.
    ```shell title="Scala"
    bin/spark-shell --packages 'com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.7'
    ```
    ```scala title="PySpark"
    bin/pyspark --packages 'com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.7'
    ```
    ```scala title="SQL"
    bin/spark-sql --packages 'com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.7'
    ```

2. Create the Dataframe using the osm.pbf format, pointing to the pbf file or folder containing pbf files.
    ```scala title="Scala"
    scala> val osmDF = spark.read.format("osm.pbf").load("<osm files path here>")
    ```
    ```python title="PySpark"
    >>> osmDF = spark.read.format("osm.pbf").load("<osm files path here>")
    ```
    ```sql title="SQL"
    spark-sql> CREATE TABLE osm USING osm.pbf LOCATION '<osm files path here>';
    ```

3. Use the created dataframe as usual, keeping in mind the schema explained previously.

   - In the next example, we are going to count the number of different primitives in
   the file. As explained in the schema, 0 are nodes, 1 ways and 2 relations.
    ```scala title="Scala"
    scala> osmDF.groupBy("type").count().show()
    +----+--------+
    |type|   count|
    +----+--------+
    |   1| 2096455|
    |   2|   91971|
    |   0|19426617|
    +----+--------+
    ```
    ```python title="PySpark"
    >>> osmDF.groupBy("type").count().show()
    +----+--------+
    |type|   count|
    +----+--------+
    |   1| 2096455|
    |   2|   91971|
    |   0|19426617|
    +----+--------+
    ```
    ```sql title="SQL"
    spark-sql> select type, count(type) from osm group by type
    1   338795
    2   10357
    0   2328075
    ```

    - In this other examples, we are going to extract all traffic lights as POIs.
    ```scala title="Scala"
    scala> osmDF.select("latitude", "longitude", "tags").where("element_at(tags, 'highway') == 'traffic_signals'").show(10,false)
    +------------------+-------------------+------------------------------------------------------------------------------+
    |latitude          |longitude          |tags                                                                          |
    +------------------+-------------------+------------------------------------------------------------------------------+
    |54.59766649999997 |-5.8889806000000045|[highway -> traffic_signals]                                                  |
    |54.58006689999997 |-5.938683200000003 |[highway -> traffic_signals, traffic_signals -> signal]                       |
    |54.58260049999997 |-5.946187600000005 |[direction -> backward, highway -> traffic_signals, traffic_signals -> signal]|
    |51.90097769999996 |-8.470285700000005 |[highway -> traffic_signals]                                                  |
    |51.901616299999965|-8.470139700000004 |[highway -> traffic_signals]                                                  |
    |51.89978239999997 |-8.465829200000002 |[highway -> traffic_signals]                                                  |
    |51.89707529999997 |-8.474892800000001 |[highway -> traffic_signals]                                                  |
    |51.89784849999997 |-8.466895200000002 |[highway -> traffic_signals]                                                  |
    |51.89547809999997 |-8.476100900000002 |[highway -> traffic_signals]                                                  |
    |51.89772569999997 |-8.477145100000003 |[highway -> traffic_signals]                                                  |
    +------------------+-------------------+------------------------------------------------------------------------------+
    only showing top 10 rows
    ```
    ```python title="PySpark"
    >>> osmDF.select("latitude", "longitude", "tags").where("element_at(tags, 'highway') == 'traffic_signals'").show(10,False)
    +------------------+-------------------+------------------------------------------------------------------------------+
    |latitude          |longitude          |tags                                                                          |
    +------------------+-------------------+------------------------------------------------------------------------------+
    |54.59766649999997 |-5.8889806000000045|[highway -> traffic_signals]                                                  |
    |54.58006689999997 |-5.938683200000003 |[highway -> traffic_signals, traffic_signals -> signal]                       |
    |54.58260049999997 |-5.946187600000005 |[direction -> backward, highway -> traffic_signals, traffic_signals -> signal]|
    |51.90097769999996 |-8.470285700000005 |[highway -> traffic_signals]                                                  |
    |51.901616299999965|-8.470139700000004 |[highway -> traffic_signals]                                                  |
    |51.89978239999997 |-8.465829200000002 |[highway -> traffic_signals]                                                  |
    |51.89707529999997 |-8.474892800000001 |[highway -> traffic_signals]                                                  |
    |51.89784849999997 |-8.466895200000002 |[highway -> traffic_signals]                                                  |
    |51.89547809999997 |-8.476100900000002 |[highway -> traffic_signals]                                                  |
    |51.89772569999997 |-8.477145100000003 |[highway -> traffic_signals]                                                  |
    +------------------+-------------------+------------------------------------------------------------------------------+
    only showing top 10 rows
    ```
    ```sql title="SQL"
    spark-sql> select latitude, longitude, tags from osm where type = 0 and element_at(tags, "highway") == 'traffic_signals' limit 10;
    40.42125            -3.6844500000000004 {"crossing":"traffic_signals","crossing_ref":"zebra","highway":"traffic_signals"}
    40.41779000000001   -3.6241199999999996 {"highway":"traffic_signals"}
    40.41473000000003   -3.627109999999999  {"highway":"traffic_signals"}
    40.414200000000015  -3.6282099999999993 {"highway":"traffic_signals"}
    40.42635999999994   -3.727220000000005  {"crossing":"traffic_signals","highway":"traffic_signals"}
    40.41937999999995   -3.688820000000004  {"highway":"traffic_signals"}
    40.426489999999944  -3.687640000000004  {"highway":"traffic_signals","traffic_signals":"signal"}
    40.421339999999944  -3.683020000000004  {"highway":"traffic_signals"}
    40.41797999999994   -3.669340000000004  {"highway":"traffic_signals"}
    40.418319999999945  -3.6762200000000043 {"highway":"traffic_signals","traffic_signals":"signal"}
    Time taken: 0.128 seconds, Fetched 10 row(s)
    ```

### Notebook

There are different notebooks solutions in the market and each one is using a different way to import libraries, but after
importing the library, you can use the osm4scala connector in the same way.

For this section, we are going to talk about [Jupyter Notebook](https://jupyter.org)


## As Scala dependency


For example,
- If you want to import the Spark Connector for Scala 2.11 and Spark 2.4: `com.acervera.osm4scala:osm4scala-spark2-shaded_2.11:1.0.7`
- If you want to import the Spark Connector for Scala 2.12 and Spark 2.4: `com.acervera.osm4scala:osm4scala-spark2-shaded_2.12:1.0.7`
- If you want to import the Spark Connector for Scala 2.12 and Spark 3.0: `com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.7`



## Why a fat shaded library?

Osm4scala has a transitive dependency with Java Google Protobuf library. Spark, Hadoop and other libraries in the
ecosystem are using an old version of the same library (currently v2.5.0 from Mar, 2013) that is not compatible.

To solve the conflict, I published the library in two fashion:
- Fat and Shaded as `osm4scala-spark3-shaded` that solves `com.google.protobuf.**` conflicts.
- Don't shaded as `osm4scala-spark3`, so you can solve the conflict on your way.

